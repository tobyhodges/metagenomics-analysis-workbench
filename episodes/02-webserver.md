---
Title: "Metagenomic Cloud Services"
Teaching: 5 minutes
Exercises: 15
Questions:
- "What are cloud services?"
- "How can I analyze metagenomic data with them?"
objectives:
- "Explore our data in a metagenomic web server."
- "Log into a remote machine through command line."
- "Understand the differences between local and web servers."  
keypoints:
- "There are web services for metagenomic analysis, like MG-RAST."
- "AWS is a computer cloud instance that can be used to do metagenomic analyzes."
- "Web services are easy to use, but cannot be tweaked as much."
- "AWS and local computers allow a greater degree of personalization."

---


## Cloud pipelines can be web or command-line based
The cloud is that place to where we can send our data to be stored and analyzed. 
To access cloud services, we can use traditional web pages (think of Dropbox or Google Drive), or 
use a more direct connecction through the command line. In this lesson we explore both approaches
to upload, store and analyze our metagenomic data.

For the web bounded cloud services we will use MG-RAST, an online metagenomic plataform where 
you can upload your raw data with its corresponding metadata and get a full taxonomic analysis of
it. MG-RAST is a great place to get started in this type of analyzes and it is also a big repository of 
available data for future experiments. On the downside, it is not possible to greaty modifiy the steps 
and parameters in the MG-RAST workflow, so there is not much leeway when it comes to implement our 
prefered analysis tools when using MG-RAST.

On the other hand, we have the other type of cloud services, like AWS. These, in contrast to 
web-bound services like MG-RAST, are much more flexibe, since they are basically powerful computers 
to which we access remotely. The downside here is that we access these cloud services 
through the command line, so there is practically no graphical interface, which can be a little bit 
jarring if you're not used to work through text commands alone. 

In short, command line workflows are more flexible and adaptable to individual needs, 
but automated web servers can quickly and easly give us a idea of the content of our data. 
So easly and quickly that, in fact, our Cuatro Ciénegas data is already in MG-RAST! 
You can check it out [here](https://www.mg-rast.org/mgmain.html?mgpage=project&project=mgp96823). 

## Cuatro Ciénegas in MG-RAST  

Let's check the taxonomical distribution of our sample first. If you look at the MG-RAST charts, 
we can see that our Cuatro Cienegas sample is mostly bacteria.  

<a href="{{ page.root }}/fig/md-02-mgm4913055.3_domain.png">
  <img src="{{ page.root }}/fig/md-02-mgm4913055.3_domain.png" alt="Domain" />
</a>


The most abundant phylum is Proteobacteria.  
<a href="{{ page.root }}/fig/md-02-mgm4913055.3_phylum.png">
  <img src="{{ page.root }}/fig/md-02-mgm4913055.3_phylum.png" alt="Phylum" />
</a>

 And going even further, we can see that the most abundant genera is *Erythrobacter*. 
 <a href="{{ page.root }}/fig/md-02-mgm4913055.3_genus.png">
  <img src="{{ page.root }}/fig/md-02-mgm4913055.3_genus.png" alt="Genus" />
</a>

Since we have a shotgun metagenome, we can also investigate the metabolic functions 
present in our sample. MG-RAST can find genes and annotate their function through 
an implementation of RAST, or Rapid Annotation using Subsystems Technology. 
By looking at the charts generated by this analysis, we see that most of the genes
are dedicated to metabolism.  

<a href="{{ page.root }}/fig/md-02-mgm4913055.3_cog.png">
  <img src="{{ page.root }}/fig/md-02-mgm4913055.3_cog.png" alt="Cog Metagenome" />
</a>

<a href="{{ page.root }}/fig/md-02-mgm4913055.3_subsystems.png">
  <img src="{{ page.root }}/fig/md-02-mgm4913055.3_subsystems.png" alt="Subsystems" />
</a>

 <a href="{{ page.root }}/fig/md-02-mgm4913055.3_predicted_features.png">
  <img src="{{ page.root }}/fig/md-02-mgm4913055.3_predicted_features.png" alt="Predicted features" />
</a>
 
<a href="{{ page.root }}/fig/md-02-mgm4913055.3_source_hits_distribution.png">
  <img src="{{ page.root }}/fig/md-02-mgm4913055.3_source_hits_distribution.png" alt="Source Hits" />
</a>

> ## Exercise
> 
> We saw the piecharts for kingdom, phylum and gnera, but what about family. Which family is the most abundant?
> 
> 
>> ## Solution
>>  The piechart from MG-RAST shows that *Rhodobacteraceae* is the most abundant family. 
>> 
> {: .solution}
{: .challenge}


## AWS is a command line cloud server 
The cloud computer we are going to use is provided by Amazon Web Services (AWS). It is already equiped with 
all of the metagenomic analysis command line utilities needed fot this workshop. To use it, we have to open our command line
(it should be easly accesible in any Linux distribution and OSX10; for Windows, you can install wsl2. Please ask one of the
helpers if you cannot find your command line) and then type the commands to log into the service and move files between  
your remote and your local computer.  


~~~
$ ssh dcuser@ec2-3-238-253-45.compute-1.amazonaws.com 
~~~
{: .bash} 

~~~
Welcome to Ubuntu 14.04.3 LTS (GNU/Linux 3.13.0-48-generic x86_64)                                                                                                     * 
Documentation:  https://help.ubuntu.com/                                                                                                                             
System information as of Fri Nov 27 06:29:17 UTC 2020 
~~~
{: .output}

We need to know where are we located in the AWS machine, to know the direction 
of the files that we are going to copy between the AWS machine and your local machine. 
To do this, we can check the current directory with `pwd`.
~~~
$ pwd 
~~~
{: .bash}  
~~~
$ /home/dcuser  
~~~
{: .output}

We are inside a directory called dcuser, which is, itself, insde the home directory.

Now, let's check what files do we have in our current working directory (dcuser) with `ls`.

~~~
$ ls 
~~~
{: .bash}  
~~~
$ dc_workshop.tar.gz  
~~~
{: .output}

This file is compressed, so we need to decompress it first.
To do this we can use the `tar` command.  
~~~
$ tar -xzf ~/dc_workshop.tar.gz 
~~~
{: .bash}  

> ## Exercise
> 
> Now that we have decompressd our file, how can we check what files were extracted?
> 
> 
>> ## Solution
>> We can use `ls dc_workshop/*` again 
>> 
> {: .solution}
{: .challenge}

Let's go back to our local machine. To log out from the remote machine you can use `exit`.  
~~~
$ exit
~~~
{: .bash}

If you now ask the terminal to print the working directory with `pwd` 
it will show your local working directory. That's how we know we're back to it. 
To copy files between your computer and the remote computer, we will use the `scp` command.  
The general syntax to use it is this would be as follows: 
~~~
$ scp <where is the file> <where you want the file to be>  
~~~
{: .output}  

For example, you have the metadata file `MGRAST_MetaData_JP.xlsx` in your remote machine. 
This file is located at the directory `/home/dcuser/dc_workshop/metadata/`.  To copy this file
into our local machine lets use `scp` command. 

~~~
$ scp dcuser@ec2-3-238-253-45.compute-1.amazonaws.com:/home/dcuser/dc_workshop/metadata/MGRAST_MetaData_JP.xlsx .
~~~
{: .data}  

~~~
MGRAST_MetaData_JP.xlsx                          100%   53KB 164.8KB/s   00:00  
~~~
{: .output}  


> ## Exercise 1 copy local files into AWS remote instance
> 
> What would be the correct syntax to upload some local file named `APJ4_MetaData_JP.xlsx.` 
> into you AWS remote instance?  
>
>   a) ssh dcuser@ec2-3-238-253-45.compute-1.amazonaws.com:/home/dcuser/. APJ4_MetaData_JP.xlsx  
>
>   b) ssh APJ4_MetaData_JP.xlsx dcuser@ec2-3-238-253-45.compute-1.amazonaws.com:/home/dcuser/.  
>
>   c) scp APJ4_MetaData_JP.xlsx dcuser@ec2-3-238-253-45.compute-1.amazonaws.com:/home/dcuser/.  
>
>> ## Solution  
>> ~~~ 
>> $  scp APJ4_MetaData_JP.xlsx dcuser@ec2-3-238-253-45.compute-1.amazonaws.com:/home/dcuser/.
>> ~~~
>> {: .bash}
>> ~~~ 
>> c option is the only one that uses secure copy command.   
>> ~~~
>> {: .output}
> {: .solution}
{: .challenge}


> ## Exercise 2 Depth of the sample  
>  At what depth was the sample collected?
>> ## Solution
>> R- 0.165. Either open the metadata excell file in your local computer or go to 
>> the MgRAST website of the project. 
> {: .solution}
{: .challenge}

> ## Exercise 3 Your own project  
>  You can find metagenomic data in your AWS remote instance   
> located at /home/dcuser/dc_workshop/assembly/JP4D.fasta   
>  With this data upload your own project to mgRAST. What else do you need?      
>> ## Solution
>> MgRAST will ask you for a metadata file and genomic data. 
>>
>> The metadata file has been previously downloaded in this lesson.    
>>
>> To download the genomic file use `scp`      
>>
>> ~~~
>>  scp dcuser@ec2-3-238-253-45.compute-1.amazonaws.com:/home/dcuser/dc_workshop/assembly/JP4D.fasta .
>> ~~~
>> {: .bash}
>>
>> Upload this files to your MG-RAST account.  
> {: .solution}
{: .challenge}

> ## `.callout`
>
> To analize data from a metagenome experiment, web and command line based strategies are available, they can complement each other.
{: .callout}


> ## `.discussion`
>
> If you have to analize data from 200 metagenomic samples, which kind of strategy would you use?
{: .discussion}

